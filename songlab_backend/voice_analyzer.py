"""
Advanced Voice Analyzer - ê¸°ì¡´ VoiceAnalyzer í˜¸í™˜ ë²„ì „
4ì¶• ê³ ê¸‰ ë³´ì»¬ ë¶„ì„ ì‹œìŠ¤í…œ (Praat + Torchaudio + PyLoudnorm)
ê¸°ì¡´ API í˜¸í™˜ì„± ìœ ì§€í•˜ë©´ì„œ ë¶„ì„ ì •í™•ë„ ëŒ€í­ í–¥ìƒ
"""

import numpy as np
import torch
import torchaudio
import torchaudio.transforms as T
import torchaudio.functional as F
from typing import Dict, Tuple, Optional
import parselmouth
from parselmouth.praat import call
import pyloudnorm as pyln
import soundfile as sf
import io
import warnings
warnings.filterwarnings('ignore')


class VoiceAnalyzer:
    """
    ê³ ê¸‰ ìŒì„± ë¶„ì„ê¸° - ê¸°ì¡´ API í˜¸í™˜ì„± ìœ ì§€
    """
    
    def __init__(self):
        self.sr = 22050  # ê³ í’ˆì§ˆ ë¶„ì„ì„ ìœ„í•´ 22050Hz ì‚¬ìš©
        self.setup_processors()
        
        # ê¸°ì¡´ API í˜¸í™˜ì„ ìœ„í•œ ë§¤í•‘
        self.axis_mapping = {
            'brightness': 'brightness',
            'thickness': 'thickness', 
            'clarity': 'adduction',  # clarity â†’ adduction (ì„±ëŒ€ë‚´ì „)
            'power': 'spl'           # power â†’ spl (ìŒì••)
        }
    
    def setup_processors(self):
        """í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”"""
        self.mel_transform = T.MelSpectrogram(
            sample_rate=self.sr,
            n_fft=2048,
            n_mels=128,
            hop_length=512
        )
        
        self.spectral_centroid = T.SpectralCentroid(sample_rate=self.sr)
        self.loudness_meter = pyln.Meter(self.sr)
    
    def analyze_audio(self, audio_data):
        """
        ê¸°ì¡´ API í˜¸í™˜ ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
        Returns: dict with brightness, thickness, clarity, power, potential_high_note
        """
        try:
            # ê³ ê¸‰ 4ì¶• ë¶„ì„ ìˆ˜í–‰
            advanced_results = self._analyze_audio_advanced(audio_data)
            
            # ê¸°ì¡´ API í˜•ì‹ìœ¼ë¡œ ë³€í™˜ + ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€
            legacy_results = {
                'brightness': advanced_results['brightness'],
                'thickness': advanced_results['thickness'],
                'clarity': advanced_results['adduction'],    # adduction â†’ clarity 
                'power': advanced_results['spl'],            # spl â†’ power
                'potential_high_note': advanced_results['potential_high_note'],  # ê³ ìŒ ì ì¬ë ¥ ì¶”ê°€
                'gender': advanced_results['gender']         # ì„±ë³„ ì •ë³´ë„ ì¶”ê°€
            }
            
            return legacy_results
            
        except Exception as e:
            print(f"Analysis error: {e}")
            return self._get_fallback_scores()
    
    def _analyze_audio_advanced(self, audio_data: bytes) -> Dict[str, float]:
        """ê³ ê¸‰ 4ì¶• ë¶„ì„ (ë‚´ë¶€ í•¨ìˆ˜)"""
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ ë° ê³ ê¸‰ ì „ì²˜ë¦¬
            audio = self._load_and_preprocess(audio_data)
            
            # 4ì¶• ë¶„ì„
            brightness = self._analyze_brightness(audio)
            thickness = self._analyze_thickness(audio)
            adduction = self._analyze_adduction(audio)
            spl = self._analyze_spl(audio)
            
            # ì„±ë³„ ì¶”ë¡  ë° ì ì¬ì  ê³ ìŒë ¥
            gender = self._infer_gender(brightness, thickness, spl, audio)
            potential_high_note = self._calculate_potential_high_note(
                gender, brightness, thickness, adduction, spl
            )
            
            return {
                'brightness': float(brightness),
                'thickness': float(thickness),
                'adduction': float(adduction),
                'spl': float(spl),
                'gender': gender,
                'potential_high_note': potential_high_note
            }
            
        except Exception as e:
            print(f"Advanced analysis error: {e}")
            return self._get_fallback_advanced()
    
    def _load_and_preprocess(self, audio_data: bytes) -> np.ndarray:
        """ê³ ê¸‰ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        
        audio_buffer = io.BytesIO(audio_data)
        audio, sr = sf.read(audio_buffer)
        
        # ëª¨ë…¸ ë³€í™˜
        if len(audio.shape) > 1:
            audio = np.mean(audio, axis=1)
        
        # ë¦¬ìƒ˜í”Œë§
        if sr != self.sr:
            import resampy
            audio = resampy.resample(audio, sr, self.sr)
        
        # 1. ë…¸ì´ì¦ˆ ì œê±°
        audio = self._remove_noise(audio)
        
        # 2. ë¬´ìŒ êµ¬ê°„ ì œê±°
        audio = self._trim_silence(audio, top_db=20)
        
        # 3. MR ì œê±° (ë³´ì»¬ ê°•ì¡°)
        audio = self._remove_mr(audio)
        
        # 4. ì •ê·œí™”
        audio = audio / (np.max(np.abs(audio)) + 1e-8)
        
        return audio
    
    def _remove_noise(self, audio: np.ndarray) -> np.ndarray:
        """ìŠ¤í™íŠ¸ëŸ¼ ì°¨ê°ë²• ë…¸ì´ì¦ˆ ì œê±°"""
        try:
            audio_tensor = torch.FloatTensor(audio)
            stft = torch.stft(
                audio_tensor,
                n_fft=2048,
                hop_length=512,
                window=torch.hann_window(2048),
                return_complex=True
            )
            
            magnitude = torch.abs(stft)
            phase = torch.angle(stft)
            
            # ì²« 0.5ì´ˆë¥¼ ë…¸ì´ì¦ˆë¡œ ì¶”ì •
            noise_frames = int(0.5 * self.sr / 512)
            if magnitude.shape[1] > noise_frames:
                noise_spectrum = torch.mean(magnitude[:, :noise_frames], dim=1, keepdim=True)
                clean_magnitude = magnitude - 2.0 * noise_spectrum
                clean_magnitude = torch.maximum(clean_magnitude, 0.1 * magnitude)
            else:
                clean_magnitude = magnitude
            
            # ì—­ë³€í™˜
            clean_stft = clean_magnitude * torch.exp(1j * phase)
            clean_audio = torch.istft(
                clean_stft,
                n_fft=2048,
                hop_length=512,
                window=torch.hann_window(2048)
            )
            
            return clean_audio.numpy()
            
        except Exception as e:
            print(f"Noise removal error: {e}")
            return audio
    
    def _trim_silence(self, audio: np.ndarray, top_db: int = 20) -> np.ndarray:
        """ì—ë„ˆì§€ ê¸°ë°˜ ë¬´ìŒ ì œê±°"""
        try:
            frame_length = 2048
            hop_length = 512
            
            energy = []
            for i in range(0, len(audio) - frame_length, hop_length):
                frame = audio[i:i+frame_length]
                energy.append(np.sum(frame**2))
            
            if len(energy) == 0:
                return audio
                
            energy = np.array(energy)
            energy_db = 10 * np.log10(energy + 1e-10)
            
            max_energy_db = np.max(energy_db)
            threshold = max_energy_db - top_db
            
            valid_frames = energy_db > threshold
            if not np.any(valid_frames):
                return audio
            
            valid_indices = np.where(valid_frames)[0]
            start_sample = valid_indices[0] * hop_length
            end_sample = min((valid_indices[-1] + 1) * hop_length + frame_length, len(audio))
            
            return audio[start_sample:end_sample]
            
        except Exception as e:
            print(f"Silence trimming error: {e}")
            return audio
    
    def _remove_mr(self, audio: np.ndarray) -> np.ndarray:
        """MR ì œê±° - ë³´ì»¬ ì£¼íŒŒìˆ˜ ëŒ€ì—­ ê°•ì¡°"""
        try:
            audio_tensor = torch.FloatTensor(audio)
            stft = torch.stft(
                audio_tensor,
                n_fft=2048,
                hop_length=512,
                window=torch.hann_window(2048),
                return_complex=True
            )
            
            magnitude = torch.abs(stft)
            phase = torch.angle(stft)
            
            freqs = torch.linspace(0, self.sr // 2, magnitude.shape[0])
            vocal_mask = torch.ones_like(magnitude)
            
            # 80Hz ì´í•˜ ì•½í™” (ì €ìŒ ì•…ê¸°)
            low_freq_mask = freqs < 80
            vocal_mask[low_freq_mask, :] *= 0.3
            
            # 8000Hz ì´ìƒ ì•½í™”
            high_freq_mask = freqs > 8000
            vocal_mask[high_freq_mask, :] *= 0.5
            
            # ë³´ì»¬ ì£¼íŒŒìˆ˜ 200-2000Hz ê°•ì¡°
            vocal_freq_mask = (freqs >= 200) & (freqs <= 2000)
            vocal_mask[vocal_freq_mask, :] *= 1.2
            
            enhanced_magnitude = magnitude * vocal_mask
            
            enhanced_stft = enhanced_magnitude * torch.exp(1j * phase)
            enhanced_audio = torch.istft(
                enhanced_stft,
                n_fft=2048,
                hop_length=512,
                window=torch.hann_window(2048)
            )
            
            return enhanced_audio.numpy()
            
        except Exception as e:
            print(f"MR removal error: {e}")
            return audio
    
    def _analyze_brightness(self, audio: np.ndarray) -> float:
        """ë°ê¸° ë¶„ì„: Spectral centroid + Formant"""
        try:
            audio_tensor = torch.FloatTensor(audio).unsqueeze(0)
            
            # Spectral centroid ê³„ì‚°
            window = torch.hann_window(2048)
            stft = torch.stft(
                audio_tensor.squeeze(),
                n_fft=2048,
                hop_length=512,
                window=window,
                return_complex=True
            )
            magnitude = torch.abs(stft)
            
            freqs = torch.linspace(0, self.sr // 2, magnitude.shape[0])
            
            centroids = []
            for t in range(magnitude.shape[1]):
                mag_frame = magnitude[:, t]
                if mag_frame.sum() > 1e-10:
                    centroid = torch.sum(freqs * mag_frame) / torch.sum(mag_frame)
                    if not torch.isnan(centroid) and not torch.isinf(centroid):
                        centroids.append(centroid.item())
            
            if centroids:
                centroid_mean = np.mean(centroids)
            else:
                centroid_mean = 2000
            
            spectral_brightness = (centroid_mean - 2500) / 25
            
            # Praat í¬ë¨¼íŠ¸ ë¶„ì„
            try:
                sound = parselmouth.Sound(audio, sampling_frequency=self.sr)
                formant = sound.to_formant_burg()
                duration = sound.duration
                mid_time = duration / 2
                
                f1 = call(formant, "Get value at time", 1, mid_time, "Hertz", "Linear")
                f2 = call(formant, "Get value at time", 2, mid_time, "Hertz", "Linear")
                
                if (not np.isnan(f1) and not np.isnan(f2) and 
                    f1 > 0 and f2 > 0 and f1 < 2000 and f2 < 4000):
                    formant_brightness = (f2 / f1 - 1.5) * 50
                else:
                    formant_brightness = 0
            except:
                formant_brightness = 0
            
            brightness = 0.6 * spectral_brightness + 0.4 * formant_brightness
            
            if np.isnan(brightness) or np.isinf(brightness):
                brightness = 0.0
            
            return float(np.clip(brightness, -100, 100))
            
        except Exception as e:
            print(f"Brightness analysis error: {e}")
            return 0.0
    
    def _analyze_thickness(self, audio: np.ndarray) -> float:
        """ë‘ê»˜ ë¶„ì„: Harmonic richness + Spectral complexity"""
        try:
            audio_tensor = torch.FloatTensor(audio)
            
            # Mel spectrogram energy distribution
            mel_spec = self.mel_transform(audio_tensor)
            low_freq_energy = torch.mean(mel_spec[:32, :])
            high_freq_energy = torch.mean(mel_spec[64:, :])
            freq_balance = (low_freq_energy - high_freq_energy) * 30
            
            # Spectral complexity
            stft = torch.stft(
                audio_tensor,
                n_fft=2048,
                hop_length=512,
                return_complex=True
            )
            magnitude = torch.abs(stft)
            spectral_variance = torch.var(magnitude, dim=0).mean()
            complexity_score = torch.log(1 + spectral_variance * 10).item() * 20
            
            # Harmonic richness
            fft = torch.fft.rfft(audio_tensor)
            fft_mag = torch.abs(fft)
            peaks, _ = torch.topk(fft_mag, 20)
            harmonic_richness = (peaks[1:].mean() / (peaks[0] + 1e-8)).item() * 50
            
            thickness = 0.4 * freq_balance + 0.3 * complexity_score + 0.3 * harmonic_richness
            
            return float(np.clip(thickness, -100, 100))
            
        except Exception as e:
            print(f"Thickness analysis error: {e}")
            return 0.0
    
    def _analyze_adduction(self, audio: np.ndarray) -> float:
        """ì„±ëŒ€ë‚´ì „ ë¶„ì„: HNR + Jitter + Shimmer"""
        try:
            sound = parselmouth.Sound(audio, sampling_frequency=self.sr)
            
            # HNR (Harmonic-to-Noise Ratio)
            harmonicity = sound.to_harmonicity()
            hnr_values = harmonicity.values
            hnr_mean = np.mean(hnr_values[~np.isnan(hnr_values)])
            hnr_score = np.clip((hnr_mean - 3) * 15, -50, 50)
            
            # Jitter
            try:
                pitch = sound.to_pitch()
                jitter_local = call(pitch, "Get jitter (local)", 0, 0, 0.0001, 0.02, 1.3)
                jitter_score = max(50 - jitter_local * 1000, -50)
            except:
                jitter_score = 0
            
            # Shimmer
            try:
                shimmer_local = call(sound, "Get shimmer (local)", 0, 0, 0.0001, 0.02, 1.3, 1.6)
                shimmer_score = max(50 - shimmer_local * 100, -50)
            except:
                shimmer_score = 0
            
            adduction = 0.3 * hnr_score + 0.35 * jitter_score + 0.35 * shimmer_score
            
            return float(np.clip(adduction, -100, 100))
            
        except Exception as e:
            print(f"Adduction analysis error: {e}")
            return 0.0
    
    def _analyze_spl(self, audio: np.ndarray) -> float:
        """ìŒì•• ë¶„ì„: RMS + LUFS"""
        try:
            # RMS Energy
            rms_energy = np.sqrt(np.mean(audio**2))
            rms_db = 20 * np.log10(rms_energy + 1e-8)
            rms_score = (rms_db + 60) * 100 / 60
            
            # LUFS
            try:
                if len(audio) / self.sr >= 0.4:
                    lufs = self.loudness_meter.integrated_loudness(audio)
                    lufs_score = (lufs + 50) * 100 / 40
                else:
                    lufs_score = rms_score
            except:
                lufs_score = rms_score
            
            # Dynamic Range
            peak_level = 20 * np.log10(np.max(np.abs(audio)) + 1e-8)
            dynamic_range = peak_level - rms_db
            dynamic_score = min(dynamic_range * 5, 50)
            
            spl = 0.6 * rms_score + 0.3 * lufs_score + 0.1 * dynamic_score
            
            return float(np.clip(spl, -100, 100))
            
        except Exception as e:
            print(f"SPL analysis error: {e}")
            return 0.0
    
    def _infer_gender(self, brightness: float, thickness: float, spl: float, audio: np.ndarray) -> str:
        """ì„±ë³„ ì¶”ë¡ """
        try:
            sound = parselmouth.Sound(audio, sampling_frequency=self.sr)
            pitch = sound.to_pitch()
            f0_values = pitch.selected_array['frequency']
            f0_values = f0_values[f0_values != 0]
            
            if len(f0_values) > 0:
                avg_f0 = np.mean(f0_values)
                
                if avg_f0 > 180:
                    return 'female'
                elif avg_f0 < 130:
                    return 'male'
                else:
                    gender_score = 0
                    if brightness > 10: gender_score += 1
                    if thickness < 0: gender_score += 1
                    if spl < 20: gender_score += 1
                    return 'female' if gender_score >= 2 else 'male'
            else:
                return 'unknown'
                
        except:
            return 'unknown'
    
    def generate_mbti_style(self, scores):
        """
        Generate voice type classification based on 4-axis scores
        """
        # Map scores to voice characteristic dimensions
        type_code = ""
        
        # Brightness: B (Bright) vs D (Dark)
        type_code += "B" if scores["brightness"] > 0 else "D"
        
        # Thickness: T (Thick) vs N (thiN)  
        type_code += "T" if scores["thickness"] > 0 else "N"
        
        # Clarity: C (Clear) vs F (Foggy/breathy)
        type_code += "C" if scores["clarity"] > 0 else "F"
        
        # Power: P (Powerful) vs S (Soft)
        type_code += "P" if scores["power"] > 0 else "S"
        
        # Define characteristics for each type
        voice_descriptions = {
            "BTCP": "ë°ê³  ë‘êº¼ìš°ë©° ëª…ë£Œí•˜ê³  íŒŒì›Œí’€í•œ ë³´ì»¬ | íŒ/ë®¤ì§€ì»¬í˜•",
            "BTCS": "ë°ê³  ë‘ê»ê³  ëª…ë£Œí•˜ì§€ë§Œ ë¶€ë“œëŸ¬ìš´ ë³´ì»¬ | íŒ/ë°œë¼ë“œí˜•",
            "BTFP": "ë°ê³  ë‘ê»ê³  ìˆ¨ì„ì¸ íŒŒì›Œí’€í•œ ë³´ì»¬ | ì†Œìš¸/ë¸”ë£¨ìŠ¤í˜•",
            "BTFS": "ë°ê³  ë‘ê»ê³  ìˆ¨ì„ì¸ ë¶€ë“œëŸ¬ìš´ ë³´ì»¬ | R&B/ë„¤ì˜¤ì†Œìš¸í˜•",
            "BNCP": "ë°ê³  ì–‡ìœ¼ë©° ëª…ë£Œí•˜ê³  íŒŒì›Œí’€í•œ ë³´ì»¬ | K-POP/ëŒ„ìŠ¤í˜•",
            "BNCS": "ë°ê³  ì–‡ê³  ëª…ë£Œí•˜ì§€ë§Œ ë¶€ë“œëŸ¬ìš´ ë³´ì»¬ | ì–´ì¿ ìŠ¤í‹±/í¬í¬í˜•",
            "BNFP": "ë°ê³  ì–‡ê³  ìˆ¨ì„ì¸ íŒŒì›Œí’€í•œ ë³´ì»¬ | ì¸ë””íŒ/ë“œë¦¼íŒí˜•",
            "BNFS": "ë°ê³  ì–‡ê³  ìˆ¨ì„ì¸ ë¶€ë“œëŸ¬ìš´ ë³´ì»¬ | ë³´ì‚¬ë…¸ë°”/ì¬ì¦ˆí˜•",
            "DTCP": "ì–´ë‘¡ê³  ë‘êº¼ìš°ë©° ëª…ë£Œí•˜ê³  íŒŒì›Œí’€í•œ ë³´ì»¬ | ë¡/ë©”íƒˆí˜•",
            "DTCS": "ì–´ë‘¡ê³  ë‘ê»ê³  ëª…ë£Œí•˜ì§€ë§Œ ë¶€ë“œëŸ¬ìš´ ë³´ì»¬ | ì»¨íŠ¸ë¦¬/ë¸”ë£¨ìŠ¤í˜•",
            "DTFP": "ì–´ë‘¡ê³  ë‘ê»ê³  ìˆ¨ì„ì¸ íŒŒì›Œí’€í•œ ë³´ì»¬ | í™í•©/íŠ¸ë©í˜•",
            "DTFS": "ì–´ë‘¡ê³  ë‘ê»ê³  ìˆ¨ì„ì¸ ë¶€ë“œëŸ¬ìš´ ë³´ì»¬ | ì¬ì¦ˆ/ì†Œìš¸í˜•",
            "DNCP": "ì–´ë‘¡ê³  ì–‡ìœ¼ë©° ëª…ë£Œí•˜ê³  íŒŒì›Œí’€í•œ ë³´ì»¬ | ì–¼í„°ë„ˆí‹°ë¸Œ/ì¸ë””ë¡í˜•",
            "DNCS": "ì–´ë‘¡ê³  ì–‡ê³  ëª…ë£Œí•˜ì§€ë§Œ ë¶€ë“œëŸ¬ìš´ ë³´ì»¬ | ì‹±ì–´ì†¡ë¼ì´í„°/í¬í¬í˜•",
            "DNFP": "ì–´ë‘¡ê³  ì–‡ê³  ìˆ¨ì„ì¸ íŒŒì›Œí’€í•œ ë³´ì»¬ | ì¼ë ‰íŠ¸ë¡œë‹‰/ì‹ ìŠ¤íŒí˜•",
            "DNFS": "ì–´ë‘¡ê³  ì–‡ê³  ìˆ¨ì„ì¸ ë¶€ë“œëŸ¬ìš´ ë³´ì»¬ | Lo-Fi/ì¹ ì•„ì›ƒí˜•"
        }
        
        # Generate training keywords based on lowest score
        # Filter out non-numeric values first
        numeric_scores = {k: v for k, v in scores.items() if isinstance(v, (int, float))}
        lowest_axis = min(numeric_scores, key=numeric_scores.get) if numeric_scores else 'brightness'
        
        keyword_map = {
            "brightness": ["í¬ë¨¼íŠ¸ ì¡°ì ˆ ë³´ì»¬ ë ˆìŠ¨", "ê³µëª… í›ˆë ¨ ë°œì„±ë²•", "ë°ì€ ìŒìƒ‰ ë§Œë“¤ê¸°"],
            "thickness": ["ì„±êµ¬ ì „í™˜ ì—°ìŠµë²•", "ë¯¹ìŠ¤ ë³´ì´ìŠ¤ í›ˆë ¨", "ìŒìƒ‰ ë‘ê»˜ ì¡°ì ˆë²•"],
            "clarity": ["ì„±ëŒ€ ë‚´ì „ ë°œì„±ë²•", "ëª…ë£Œí•œ ë°œìŒ í›ˆë ¨", "ê¹¨ë—í•œ ìŒìƒ‰ ë§Œë“¤ê¸°"],
            "power": ["ë³µì‹ í˜¸í¡ë²•", "ìŒì•• ì¡°ì ˆ í›ˆë ¨", "íŒŒì›Œí’€í•œ ë°œì„±ë²•"]
        }
        
        return {
            "typeCode": type_code,
            "typeName": type_code,
            "typeIcon": self._get_type_icon(type_code),
            "description": voice_descriptions.get(type_code, "ë…íŠ¹í•œ ê°œì„±ì˜ ë³´ì»¬"),
            "scores": scores,
            "youtubeKeywords": keyword_map.get(lowest_axis, ["ë³´ì»¬ ê¸°ì´ˆ ë°œì„±ë²•"])
        }
    
    def _get_type_icon(self, type_code):
        """
        Return emoji icon based on voice type
        """
        icon_map = {
            "B": "ğŸŒŸ", "D": "ğŸŒ™",  # Bright vs Dark
            "T": "ğŸ¸", "N": "ğŸ¹",  # Thick vs thiN
            "C": "ğŸ¯", "F": "ğŸ’«",  # Clear vs Foggy
            "P": "âš¡", "S": "ğŸŒŠ"   # Powerful vs Soft
        }
        # Return icon based on first letter (B/D)
        return icon_map.get(type_code[0], "ğŸ¤")
    
    def _calculate_potential_high_note(self, gender: str, brightness: float, thickness: float, adduction: float, spl: float) -> str:
        """ì ì¬ì  ê³ ìŒë ¥ ê³„ì‚°"""
        
        if gender == 'male':
            base_notes = ['A4', 'A#4', 'B4', 'C5', 'C#5', 'D5', 'D#5', 'E5', 'F5']
            base_idx = 4  # C#5
        elif gender == 'female':
            base_notes = ['C5', 'C#5', 'D5', 'D#5', 'E5', 'F5', 'F#5', 'G5', 'G#5', 'A5']
            base_idx = 5  # F5
        else:
            return 'E5'
        
        adjustment = 0
        
        if brightness > 50: adjustment += 3
        elif brightness > 20: adjustment += 2
        elif brightness > 0: adjustment += 1
        elif brightness < -40: adjustment -= 2
        
        if -10 <= thickness <= 20: adjustment += 1
        elif thickness < -30: adjustment += 2
        elif thickness > 50: adjustment -= 2
        
        if adduction > 60: adjustment += 3
        elif adduction > 30: adjustment += 2
        elif adduction > 0: adjustment += 1
        elif adduction < -30: adjustment -= 2
        
        if 30 <= spl <= 70: adjustment += 1
        elif spl > 80: adjustment -= 1
        elif spl < 10: adjustment -= 1
        
        final_idx = max(0, min(len(base_notes) - 1, base_idx + adjustment))
        
        return base_notes[final_idx]
    
    def _get_fallback_scores(self) -> Dict[str, float]:
        """ê¸°ì¡´ API í˜¸í™˜ ê¸°ë³¸ê°’"""
        return {
            'brightness': 0.0,
            'thickness': 0.0,
            'clarity': 0.0,
            'power': 0.0
        }
    
    def _get_fallback_advanced(self) -> Dict[str, float]:
        """ê³ ê¸‰ ë¶„ì„ ê¸°ë³¸ê°’"""
        return {
            'brightness': 0.0,
            'thickness': 0.0,
            'adduction': 0.0,
            'spl': 0.0,
            'gender': 'unknown',
            'potential_high_note': 'E5'
        }
    
    def get_advanced_results(self, audio_data):
        """ê³ ê¸‰ ë¶„ì„ ê²°ê³¼ ì§ì ‘ ì ‘ê·¼ (ìƒˆ ê¸°ëŠ¥)"""
        return self._analyze_audio_advanced(audio_data)


# ê¸°ì¡´ í˜¸í™˜ì„± í™•ì¸
if __name__ == "__main__":
    analyzer = VoiceAnalyzer()
    print("Advanced Voice Analyzer - ê¸°ì¡´ API í˜¸í™˜ ì™„ë£Œ!")
    print("ìƒˆ ê¸°ëŠ¥: get_advanced_results() ë©”ì„œë“œë¡œ ê³ ê¸‰ ë¶„ì„ ê°€ëŠ¥")