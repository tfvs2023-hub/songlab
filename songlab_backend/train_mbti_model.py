"""
MBTI ë³´ì»¬ ë¶„ë¥˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œìŠ¤í…œ
ë¼ë²¨ë§ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ 4ì¶• MBTI ì ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import librosa
from sklearn.model_selection import train_test_split
from typing import Dict, Tuple
import json
import os

class VocalMBTIDataset(Dataset):
    """
    ë³´ì»¬ MBTI ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    """
    def __init__(self, data_path: str, labels_path: str, sr=22050):
        """
        data_path: ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”
        labels_path: ë¼ë²¨ CSV íŒŒì¼ ê²½ë¡œ
        
        CSV í˜•ì‹:
        filename,brightness,thickness,clarity,power
        voice1.wav,-65,75,45,82
        voice2.wav,70,-40,85,65
        """
        self.sr = sr
        self.data_path = data_path
        
        # ë¼ë²¨ ë¡œë“œ
        self.labels_df = pd.read_csv(labels_path)
        self.filenames = self.labels_df['filename'].tolist()
        
        # MBTI ì ìˆ˜ ì •ê·œí™” (-100 ~ 100 -> -1 ~ 1)
        self.brightness = self.labels_df['brightness'].values / 100
        self.thickness = self.labels_df['thickness'].values / 100
        self.clarity = self.labels_df['clarity'].values / 100
        self.power = self.labels_df['power'].values / 100
        
    def __len__(self):
        return len(self.filenames)
    
    def __getitem__(self, idx):
        # ì˜¤ë””ì˜¤ ë¡œë“œ ë° íŠ¹ì§• ì¶”ì¶œ
        audio_path = os.path.join(self.data_path, self.filenames[idx])
        features = self.extract_features(audio_path)
        
        # ë¼ë²¨ (4ì¶• MBTI ì ìˆ˜)
        labels = torch.FloatTensor([
            self.brightness[idx],
            self.thickness[idx],
            self.clarity[idx],
            self.power[idx]
        ])
        
        return features, labels
    
    def extract_features(self, audio_path: str) -> torch.Tensor:
        """
        ì˜¤ë””ì˜¤ì—ì„œ íŠ¹ì§• ì¶”ì¶œ (MFCC + ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì§•)
        """
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio, sr = librosa.load(audio_path, sr=self.sr, mono=True)
        
        # ì „ì²˜ë¦¬
        audio = librosa.effects.trim(audio, top_db=20)[0]
        audio = librosa.util.normalize(audio)
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = []
        
        # 1. MFCC (13ì°¨ì›)
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
        mfcc_mean = np.mean(mfcc, axis=1)
        mfcc_std = np.std(mfcc, axis=1)
        features.extend(mfcc_mean)
        features.extend(mfcc_std)
        
        # 2. ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬
        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        features.append(np.mean(spectral_centroid))
        features.append(np.std(spectral_centroid))
        
        # 3. ìŠ¤í™íŠ¸ëŸ¼ ëŒ€ì—­í­
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]
        features.append(np.mean(spectral_bandwidth))
        features.append(np.std(spectral_bandwidth))
        
        # 4. ìŠ¤í™íŠ¸ëŸ¼ ë¡¤ì˜¤í”„
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
        features.append(np.mean(spectral_rolloff))
        features.append(np.std(spectral_rolloff))
        
        # 5. Zero Crossing Rate
        zcr = librosa.feature.zero_crossing_rate(audio)[0]
        features.append(np.mean(zcr))
        features.append(np.std(zcr))
        
        # 6. RMS ì—ë„ˆì§€
        rms = librosa.feature.rms(y=audio)[0]
        features.append(np.mean(rms))
        features.append(np.std(rms))
        
        # 7. í•˜ëª¨ë‹‰-í¼ì»¤ì‹œë¸Œ ë¹„ìœ¨
        harmonic, percussive = librosa.effects.hpss(audio)
        harmonic_ratio = np.mean(np.abs(harmonic)) / (np.mean(np.abs(harmonic)) + np.mean(np.abs(percussive)) + 1e-6)
        features.append(harmonic_ratio)
        
        return torch.FloatTensor(features)


class MBTIVocalNet(nn.Module):
    """
    MBTI ë³´ì»¬ ë¶„ë¥˜ ë”¥ëŸ¬ë‹ ëª¨ë¸
    """
    def __init__(self, input_size=37):
        super(MBTIVocalNet, self).__init__()
        
        # íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_size, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.3),
            
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
        )
        
        # 4ì¶• ì˜ˆì¸¡ í—¤ë“œ (ê° ì¶•ë³„ ì „ë¬¸í™”)
        self.brightness_head = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Tanh()  # -1 ~ 1 ë²”ìœ„
        )
        
        self.thickness_head = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Tanh()
        )
        
        self.clarity_head = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Tanh()
        )
        
        self.power_head = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Tanh()
        )
    
    def forward(self, x):
        # ê³µí†µ íŠ¹ì§• ì¶”ì¶œ
        features = self.feature_extractor(x)
        
        # 4ì¶• ì˜ˆì¸¡
        brightness = self.brightness_head(features)
        thickness = self.thickness_head(features)
        clarity = self.clarity_head(features)
        power = self.power_head(features)
        
        return torch.cat([brightness, thickness, clarity, power], dim=1)


def train_model(data_path: str, labels_path: str, epochs: int = 100):
    """
    ëª¨ë¸ í•™ìŠµ ë©”ì¸ í•¨ìˆ˜
    """
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = VocalMBTIDataset(data_path, labels_path)
    
    # í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬ (8:2)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    # ë°ì´í„°ë¡œë”
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = MBTIVocalNet().to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)
    
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘ - Device: {device}")
    print(f"   í•™ìŠµ ë°ì´í„°: {train_size}ê°œ, ê²€ì¦ ë°ì´í„°: {val_size}ê°œ")
    
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        # í•™ìŠµ í˜ì´ì¦ˆ
        model.train()
        train_loss = 0
        for features, labels in train_loader:
            features, labels = features.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # ê²€ì¦ í˜ì´ì¦ˆ
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for features, labels in val_loader:
                features, labels = features.to(device), labels.to(device)
                outputs = model(features)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        
        # í•™ìŠµë¥  ì¡°ì •
        scheduler.step(avg_val_loss)
        
        # ì§„í–‰ìƒí™© ì¶œë ¥
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}] Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': avg_val_loss,
            }, 'best_mbti_model.pth')
            print(f"   âœ… ìµœê³  ëª¨ë¸ ì €ì¥ (Val Loss: {avg_val_loss:.4f})")
    
    print(f"\nğŸ¯ í•™ìŠµ ì™„ë£Œ! ìµœì¢… ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
    return model


def predict_mbti(model_path: str, audio_path: str) -> Dict:
    """
    í•™ìŠµëœ ëª¨ë¸ë¡œ MBTI ì˜ˆì¸¡
    """
    # ëª¨ë¸ ë¡œë“œ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = MBTIVocalNet().to(device)
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # íŠ¹ì§• ì¶”ì¶œ
    dataset = VocalMBTIDataset('.', 'dummy.csv')  # ë”ë¯¸ ì¸ìŠ¤í„´ìŠ¤
    features = dataset.extract_features(audio_path).unsqueeze(0).to(device)
    
    # ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(features)
        predictions = outputs.squeeze().cpu().numpy() * 100  # -100 ~ 100 ë²”ìœ„ë¡œ ë³µì›
    
    return {
        'brightness': int(predictions[0]),
        'thickness': int(predictions[1]),
        'clarity': int(predictions[2]),
        'power': int(predictions[3])
    }


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    print("=" * 50)
    print("ğŸ¤ MBTI ë³´ì»¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ")
    print("=" * 50)
    
    # 1. ë°ì´í„° ì¤€ë¹„ (CSV íŒŒì¼ í˜•ì‹)
    sample_csv = """filename,brightness,thickness,clarity,power
voice1.wav,-65,75,45,82
voice2.wav,70,-40,85,65
voice3.wav,30,-60,-35,-45
voice4.wav,-45,65,70,90
"""
    
    with open('sample_labels.csv', 'w') as f:
        f.write(sample_csv)
    
    print("ğŸ“Š ìƒ˜í”Œ ë¼ë²¨ íŒŒì¼ ìƒì„±ë¨: sample_labels.csv")
    print("\ní•„ìš”í•œ ë°ì´í„°:")
    print("1. ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ (./audio_data/ í´ë”)")
    print("2. ë¼ë²¨ CSV íŒŒì¼ (filename, brightness, thickness, clarity, power)")
    print("\ní•™ìŠµ ì‹œì‘: train_model('./audio_data/', 'labels.csv', epochs=100)")