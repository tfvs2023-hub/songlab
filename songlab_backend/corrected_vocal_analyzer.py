"""
ì •í™•í•œ ë³´ì»¬ ë¶„ì„ê¸° - MR ë¶„ë¦¬ ë³´ì»¬ ìµœì í™”
ëª¨ë“  ì„¤ì •ì„ ìŠ¤í™ì— ë§ê²Œ êµì •
"""

import numpy as np
import librosa
import soundfile as sf
import parselmouth
from scipy import signal
from scipy.stats import linregress
import subprocess
import tempfile
import os
import warnings
warnings.filterwarnings('ignore')

class CorrectedVocalAnalyzer:
    def __init__(self):
        # íŒŒë¼ë¯¸í„° í”„ë¦¬ì…‹
        self.sr = 22050  # 22.05 kHz ê³ ì •
        self.n_fft = 2048
        self.hop_length = 256
        self.window = 'hann'
        
        # ëŒ€ì—­ ì œí•œ
        self.hp_freq = 80    # High-pass
        self.lp_freq = 8000  # Low-pass (ë¸Œë¦­ì›”)
        
        # Tilt ë¶„ì„ ëŒ€ì—­
        self.tilt_low = 300
        self.tilt_high = 3000
        
        # Mel í•„í„°
        self.n_mels = 40
        self.mel_low = 80
        self.mel_high = 8000
        
        # VAD ì„¤ì •
        self.vad_win_ms = 20
        self.vad_hop_ms = 10
        self.voiced_threshold = 0.2  # ìœ ì„± ë¹„ìœ¨ 20% ë¯¸ë§Œì´ë©´ Low confidence
        
        # F0 ì„¤ì • (ì„±ë³„ë³„)
        self.f0_settings = {
            'male': {'floor': 60, 'ceiling': 450},
            'female': {'floor': 100, 'ceiling': 700}
        }
        
        # ì •ê·œí™” í†µê³„ (z-scoreìš©)
        self.reference_stats = {
            'cpp_med': {'mean': 15.0, 'std': 5.0},
            'hnr_med': {'mean': 12.0, 'std': 4.0},
            'tilt_med': {'mean': -8.0, 'std': 3.0},
            'flatness_med': {'mean': 0.3, 'std': 0.1},
            'f0conf_mean': {'mean': 0.8, 'std': 0.15},
            'entropy_norm_med': {'mean': 0.6, 'std': 0.1},
            'h1h2_med': {'mean': 5.0, 'std': 3.0}
        }
    
    def analyze_vocal_file(self, audio_file, gender='auto', source_hint='unknown'):
        """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
        
        filename = os.path.basename(audio_file)
        print(f"\nğŸ¤ {filename} ë¶„ì„ ì¤‘...")
        
        try:
            # 0. ê³µí†µ ì „ì²˜ë¦¬
            audio, voiced_segments = self.preprocess_audio(audio_file)
            if audio is None:
                return {'error': 'ì „ì²˜ë¦¬ ì‹¤íŒ¨'}
            
            # ìœ ì„± ë¹„ìœ¨ ì²´í¬
            voiced_ratio = sum(len(seg) for seg in voiced_segments) / len(audio)
            low_confidence = voiced_ratio < self.voiced_threshold
            
            print(f"   ğŸ“Š ê¸¸ì´: {len(audio)/self.sr:.1f}ì´ˆ, ìœ ì„± ë¹„ìœ¨: {voiced_ratio:.1%}")
            if low_confidence:
                print("   âš ï¸ Low confidence (ìœ ì„± ë¹„ìœ¨ < 20%)")
            
            # ì„±ë³„ ìë™ ì¶”ì •
            if gender == 'auto':
                gender = self.estimate_gender(audio)
            
            # 1. F0 ë¶„ì„ (êµì •ëœ Praat ì„¤ì •)
            f0_result = self.analyze_f0_corrected(audio, gender)
            
            # 2. ê²½ëŸ‰ ì§€í‘œ ì„¸íŠ¸ ì¶”ì¶œ
            features = self.extract_light_features(audio, voiced_segments, f0_result)
            
            # 3. ì†ŒìŠ¤ë³„ ë³´ì •
            features_corrected = self.apply_source_correction(features, source_hint)
            
            # 4. ìµœì¢… ìŠ¤ì½”ì–´ë§
            clarity_score, grade, characteristics = self.calculate_final_clarity(features_corrected, low_confidence)
            
            result = {
                'clarity_score': clarity_score,
                'grade': grade,
                'characteristics': characteristics,
                'voiced_ratio': voiced_ratio,
                'low_confidence': low_confidence,
                'gender': gender,
                'raw_features': features_corrected
            }
            
            print(f"   âœ… ì„ ëª…ë„: {clarity_score:.1f} ({grade})")
            print(f"   ğŸ·ï¸ íŠ¹ì§•: {', '.join(characteristics)}")
            
            return result
            
        except Exception as e:
            print(f"   âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    def preprocess_audio(self, audio_file):
        """0. ê³µí†µ ì „ì²˜ë¦¬"""
        try:
            # M4A ë³€í™˜ (í•„ìš”ì‹œ)
            if audio_file.endswith('.m4a'):
                wav_path = self.convert_m4a_simple(audio_file)
                if wav_path:
                    audio, sr = sf.read(wav_path)
                    os.unlink(wav_path)
                else:
                    audio, sr = sf.read(audio_file)
            else:
                audio, sr = sf.read(audio_file)
            
            # ìŠ¤í…Œë ˆì˜¤ â†’ ëª¨ë…¸
            if len(audio.shape) > 1:
                audio = np.mean(audio, axis=1)
            
            # 22.05 kHz ë¦¬ìƒ˜í”Œë§
            if sr != self.sr:
                audio = librosa.resample(y=audio, orig_sr=sr, target_sr=self.sr)
            
            # ë¼ìš°ë“œë‹ˆìŠ¤ ë§¤ì¹­ (-23 LUFS) - ê°„ë‹¨ ë²„ì „
            rms = np.sqrt(np.mean(audio ** 2))
            if rms > 0:
                target_rms = 0.1  # ëŒ€ëµ -23 LUFS ìƒë‹¹
                audio = audio * (target_rms / rms)
            
            # ëŒ€ì—­ ì œí•œ (80-8000 Hz ë¸Œë¦­ì›” FIR)
            nyquist = self.sr / 2
            low = self.hp_freq / nyquist
            high = self.lp_freq / nyquist
            
            b, a = signal.butter(4, [low, high], btype='band')
            audio = signal.filtfilt(b, a, audio)
            
            # ê°„ë‹¨ VAD (ìœ ì„± êµ¬ê°„ ê²€ì¶œ)
            voiced_segments = self.simple_vad(audio)
            
            return audio, voiced_segments
            
        except Exception as e:
            print(f"ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None, []
    
    def convert_m4a_simple(self, m4a_path):
        """M4A â†’ WAV ë³€í™˜"""
        try:
            temp_wav = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
            temp_wav_path = temp_wav.name
            temp_wav.close()
            
            cmd = [
                'ffmpeg', '-i', m4a_path,
                '-ac', '1', '-ar', str(self.sr),
                temp_wav_path, '-y'
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            return temp_wav_path if result.returncode == 0 else None
            
        except:
            return None
    
    def simple_vad(self, audio):
        """ê°„ë‹¨ VAD (ì—ë„ˆì§€ + ZCR í•˜ì´ë¸Œë¦¬ë“œ)"""
        try:
            win_samples = int(self.vad_win_ms * self.sr / 1000)
            hop_samples = int(self.vad_hop_ms * self.sr / 1000)
            
            voiced_segments = []
            
            for i in range(0, len(audio) - win_samples, hop_samples):
                frame = audio[i:i + win_samples]
                
                # ì—ë„ˆì§€
                energy = np.sum(frame ** 2)
                
                # ZCR
                zcr = np.sum(np.diff(np.sign(frame)) != 0) / len(frame)
                
                # ê°„ë‹¨í•œ ì„ê³„ê°’ (ê²½í—˜ì )
                if energy > 0.001 and zcr < 0.3:  # ìœ ì„±ìŒ ì¡°ê±´
                    voiced_segments.append((i, i + win_samples))
            
            return voiced_segments
            
        except:
            return [(0, len(audio))]  # ì‹¤íŒ¨ì‹œ ì „ì²´ êµ¬ê°„
    
    def estimate_gender(self, audio):
        """ì„±ë³„ ìë™ ì¶”ì •"""
        try:
            # ê°„ë‹¨í•œ F0 ì¶”ì •ìœ¼ë¡œ ì„±ë³„ íŒë‹¨
            f0 = librosa.yin(audio, fmin=60, fmax=700, frame_length=2048)
            valid_f0 = f0[f0 > 0]
            
            if len(valid_f0) > 0:
                median_f0 = np.median(valid_f0)
                return 'female' if median_f0 > 180 else 'male'
            else:
                return 'male'  # ê¸°ë³¸ê°’
                
        except:
            return 'male'
    
    def analyze_f0_corrected(self, audio, gender):
        """1. êµì •ëœ F0 ë¶„ì„"""
        try:
            # Praat ì„¤ì •
            settings = self.f0_settings[gender]
            
            # Praat Sound ê°ì²´ ìƒì„±
            sound = parselmouth.Sound(audio, self.sr)
            
            # êµì •ëœ íŒŒë¼ë¯¸í„°ë¡œ F0 ì¶”ì¶œ
            pitch = sound.to_pitch(
                time_step=0.01,
                pitch_floor=settings['floor'],
                pitch_ceiling=settings['ceiling'],
                voicing_threshold=0.6,
                octave_cost=0.01,
                octave_jump_cost=0.35,
                voiced_unvoiced_cost=0.2
            )
            
            # ê²°ê³¼ ì¶”ì¶œ
            f0_values = pitch.selected_array['frequency']
            voiced_frames = f0_values > 0
            confidence = np.sum(voiced_frames) / len(f0_values) if len(f0_values) > 0 else 0
            
            # ê³ ì‹ ë¢°ë„ í”„ë ˆì„ë§Œ (conf â‰¥ 0.7)
            high_conf_mask = confidence >= 0.7
            
            return {
                'f0_values': f0_values,
                'voiced_frames': voiced_frames,
                'confidence': confidence,
                'high_conf_mask': high_conf_mask,
                'pitch_object': pitch
            }
            
        except Exception as e:
            print(f"F0 ë¶„ì„ ì˜¤ë¥˜: {e}")
            # ë°±ì—…: pYIN ì‚¬ìš©
            try:
                settings = self.f0_settings[gender]
                f0 = librosa.yin(audio, 
                                fmin=settings['floor'], 
                                fmax=settings['ceiling'], 
                                frame_length=2048)
                voiced_frames = f0 > 0
                confidence = np.sum(voiced_frames) / len(f0) if len(f0) > 0 else 0
                
                return {
                    'f0_values': f0,
                    'voiced_frames': voiced_frames,
                    'confidence': confidence,
                    'high_conf_mask': confidence >= 0.7,
                    'pitch_object': None
                }
            except:
                return {
                    'f0_values': np.array([]),
                    'voiced_frames': np.array([]),
                    'confidence': 0.0,
                    'high_conf_mask': False,
                    'pitch_object': None
                }
    
    def extract_light_features(self, audio, voiced_segments, f0_result):
        """2. ê²½ëŸ‰ ì§€í‘œ ì„¸íŠ¸ ì¶”ì¶œ"""
        features = {}
        
        try:
            # ìœ ì„± êµ¬ê°„ë§Œ ì¶”ì¶œ
            voiced_audio = []
            for start, end in voiced_segments:
                voiced_audio.extend(audio[start:end])
            
            if len(voiced_audio) == 0:
                voiced_audio = audio  # ë°±ì—…
            
            voiced_audio = np.array(voiced_audio)
            
            # STFT ê³„ì‚°
            stft = librosa.stft(voiced_audio, n_fft=self.n_fft, hop_length=self.hop_length, window=self.window)
            magnitude = np.abs(stft)
            power = magnitude ** 2
            freqs = librosa.fft_frequencies(sr=self.sr, n_fft=self.n_fft)
            
            # 1. CPP (Cepstral Peak Prominence)
            features['cpp_med'] = self.calculate_cpp_light(voiced_audio)
            
            # 2. HNR (Harmonics-to-Noise Ratio)
            features['hnr_med'] = self.calculate_hnr_corrected(voiced_audio, f0_result)
            
            # 3. Spectral Tilt (300-3000 Hz, ì•ˆì „ ë ˆì‹œí”¼)
            features['tilt_med'] = self.calculate_spectral_tilt_safe(power, freqs)
            
            # 4. Spectral Flatness (median, 80-8000 Hz)
            features['flatness_med'] = self.calculate_spectral_flatness_safe(power, freqs)
            
            # 5. F0 confidence (mean)
            features['f0conf_mean'] = f0_result['confidence']
            
            # 6. H1-H2
            features['h1h2_med'] = self.calculate_h1h2(voiced_audio, f0_result)
            
            # 7. Mel-entropy (H_norm)
            features['entropy_norm_med'] = self.calculate_mel_entropy_norm(voiced_audio)
            
            return features
            
        except Exception as e:
            print(f"íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                'cpp_med': 10.0,
                'hnr_med': 10.0,
                'tilt_med': -5.0,
                'flatness_med': 0.3,
                'f0conf_mean': 0.5,
                'h1h2_med': 5.0,
                'entropy_norm_med': 0.6
            }
    
    def calculate_cpp_light(self, audio):
        """CPP ê³„ì‚° (ê²½ëŸ‰ ë²„ì „)"""
        try:
            # ì¼€í”„ìŠ¤íŠ¸ëŸ¼ ê³„ì‚°
            spectrum = np.fft.rfft(audio)
            log_spectrum = np.log(np.abs(spectrum) + 1e-10)
            cepstrum = np.fft.irfft(log_spectrum)
            
            # 2-20ms ë²”ìœ„ì—ì„œ í”¼í¬ ì°¾ê¸°
            start_idx = int(0.002 * self.sr)  # 2ms
            end_idx = int(0.02 * self.sr)     # 20ms
            
            if end_idx < len(cepstrum):
                peak_value = np.max(cepstrum[start_idx:end_idx])
                baseline = np.mean(cepstrum[start_idx:end_idx])
                cpp = peak_value - baseline
                return max(0, min(30, cpp * 1000))  # 0-30 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§
            else:
                return 10.0
                
        except:
            return 10.0
    
    def calculate_hnr_corrected(self, audio, f0_result):
        """HNR ê³„ì‚° (êµì •ëœ ë²„ì „)"""
        try:
            if f0_result['pitch_object'] is not None:
                # Praatìœ¼ë¡œ HNR ê³„ì‚°
                harmonicity = f0_result['pitch_object'].to_harmonicity()
                hnr_values = harmonicity.values
                hnr_values = hnr_values[~np.isnan(hnr_values)]
                
                if len(hnr_values) > 0:
                    return np.median(hnr_values)
            
            # ë°±ì—…: ìê¸°ìƒê´€ ê¸°ë°˜ HNR
            autocorr = np.correlate(audio, audio, mode='full')
            autocorr = autocorr[len(autocorr)//2:]
            
            if len(autocorr) > self.sr//50:
                max_corr = np.max(autocorr[self.sr//400:self.sr//50])
                noise_level = 1 - max_corr / (autocorr[0] + 1e-10)
                hnr = 10 * np.log10((max_corr + 1e-10) / (noise_level + 1e-10))
                return max(0, min(30, hnr))
            else:
                return 10.0
                
        except:
            return 10.0
    
    def calculate_spectral_tilt_safe(self, power, freqs):
        """Spectral Tilt ê³„ì‚° (ì•ˆì „ ë ˆì‹œí”¼)"""
        try:
            # 300-3000 Hz ëŒ€ì—­ ë§ˆìŠ¤í¬
            mask = (freqs >= self.tilt_low) & (freqs <= self.tilt_high)
            
            if np.sum(mask) < 10:
                return -5.0
            
            freq_range = freqs[mask]
            
            # í”„ë ˆì„ë³„ ê¸°ìš¸ê¸° ê³„ì‚°
            tilt_values = []
            
            for frame_idx in range(power.shape[1]):
                power_frame = power[mask, frame_idx]
                
                if np.sum(power_frame) > 0:
                    log_power = 10 * np.log10(power_frame + 1e-10)
                    log_freq = np.log10(freq_range + 1e-10)
                    
                    slope, _, _, _, _ = linregress(log_freq, log_power)
                    tilt_db_oct = slope * np.log10(2)  # dB/octave
                    tilt_values.append(tilt_db_oct)
            
            # Median ë°˜í™˜
            if len(tilt_values) > 0:
                return np.median(tilt_values)
            else:
                return -5.0
                
        except:
            return -5.0
    
    def calculate_spectral_flatness_safe(self, power, freqs):
        """Spectral Flatness ê³„ì‚° (80-8000 Hz)"""
        try:
            mask = (freqs >= self.mel_low) & (freqs <= self.mel_high)
            
            if np.sum(mask) < 10:
                return 0.3
            
            flatness_values = []
            
            for frame_idx in range(power.shape[1]):
                power_frame = power[mask, frame_idx]
                
                if np.sum(power_frame) > 0:
                    geometric_mean = np.exp(np.mean(np.log(power_frame + 1e-10)))
                    arithmetic_mean = np.mean(power_frame)
                    flatness = geometric_mean / (arithmetic_mean + 1e-10)
                    flatness_values.append(flatness)
            
            if len(flatness_values) > 0:
                return np.median(flatness_values)
            else:
                return 0.3
                
        except:
            return 0.3
    
    def calculate_h1h2(self, audio, f0_result):
        """H1-H2 ê³„ì‚°"""
        try:
            if len(f0_result['f0_values']) == 0:
                return 5.0
            
            # F0ê°€ ìˆëŠ” í”„ë ˆì„ë“¤ì˜ H1-H2 ê³„ì‚°
            valid_f0 = f0_result['f0_values'][f0_result['voiced_frames']]
            
            if len(valid_f0) == 0:
                return 5.0
            
            median_f0 = np.median(valid_f0)
            
            # ìŠ¤í™íŠ¸ëŸ¼ì—ì„œ H1, H2 ì¶”ì •
            spectrum = np.fft.rfft(audio)
            freqs = np.fft.rfftfreq(len(audio), 1/self.sr)
            magnitude = np.abs(spectrum)
            
            # H1 (ê¸°ë³¸ ì£¼íŒŒìˆ˜) ì—ë„ˆì§€
            h1_idx = np.argmin(np.abs(freqs - median_f0))
            h1_energy = magnitude[h1_idx]
            
            # H2 (2ë°° ì£¼íŒŒìˆ˜) ì—ë„ˆì§€
            h2_freq = median_f0 * 2
            h2_idx = np.argmin(np.abs(freqs - h2_freq))
            h2_energy = magnitude[h2_idx]
            
            if h2_energy > 0:
                h1h2 = 20 * np.log10((h1_energy + 1e-10) / (h2_energy + 1e-10))
                return np.clip(h1h2, -10, 20)
            else:
                return 5.0
                
        except:
            return 5.0
    
    def calculate_mel_entropy_norm(self, audio):
        """Mel-entropy ê³„ì‚° (ì •ê·œí™”)"""
        try:
            # Mel í•„í„°ë±…í¬
            mel_filters = librosa.filters.mel(sr=self.sr, n_fft=self.n_fft, 
                                             n_mels=self.n_mels, 
                                             fmin=self.mel_low, 
                                             fmax=self.mel_high)
            
            # STFT
            stft = librosa.stft(audio, n_fft=self.n_fft, hop_length=self.hop_length)
            power = np.abs(stft) ** 2
            
            # Mel ìŠ¤í™íŠ¸ë¡œê·¸ë¨
            mel_spec = mel_filters @ power
            
            entropy_values = []
            
            for frame_idx in range(mel_spec.shape[1]):
                mel_frame = mel_spec[:, frame_idx]
                
                if np.sum(mel_frame) > 0:
                    # í™•ë¥  ë¶„í¬ë¡œ ì •ê·œí™”
                    p = mel_frame / np.sum(mel_frame)
                    # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
                    h = -np.sum(p * np.log(p + 1e-10))
                    # ì •ê·œí™” (0-1 ë²”ìœ„)
                    h_norm = h / np.log(self.n_mels)
                    entropy_values.append(h_norm)
            
            if len(entropy_values) > 0:
                return np.median(entropy_values)
            else:
                return 0.6
                
        except:
            return 0.6
    
    def apply_source_correction(self, features, source_hint):
        """3. ì†ŒìŠ¤ë³„ ë³´ì • (ì§€í‘œ ë‹¨ìœ„ë¡œë§Œ)"""
        corrected = features.copy()
        
        if source_hint == 'kakaotalk':
            corrected['hnr_med'] -= 1.0
            corrected['tilt_med'] += 0.5
            corrected['flatness_med'] += 0.02
        elif source_hint == 'amr_nb':
            corrected['hnr_med'] -= 2.0
            corrected['entropy_norm_med'] += 0.05
        
        return corrected
    
    def calculate_final_clarity(self, features, low_confidence):
        """4. ìµœì¢… ìŠ¤ì½”ì–´ë§"""
        try:
            # Z-score ì •ê·œí™”
            z_scores = {}
            for key, value in features.items():
                if key in self.reference_stats:
                    stats = self.reference_stats[key]
                    z_scores[key] = (value - stats['mean']) / stats['std']
                else:
                    z_scores[key] = 0
            
            # Clarity ì›ì‹œ ì ìˆ˜
            clarity_raw = (
                z_scores['cpp_med'] + 
                z_scores['hnr_med'] - 
                z_scores['tilt_med'] - 
                z_scores['flatness_med'] +
                0.5 * z_scores['f0conf_mean'] - 
                0.5 * z_scores['entropy_norm_med'] - 
                0.5 * z_scores['h1h2_med']
            )
            
            # ì‹œê·¸ëª¨ì´ë“œ ì •ê·œí™”
            clarity_sigmoid = 100 * (1 / (1 + np.exp(-clarity_raw / 2)))
            
            # Cap ê·œì¹™ (ê°€ì„±/ìˆ¨ì„ì„)
            cap = 100
            if features['h1h2_med'] > 8 or features['hnr_med'] < 10:
                cap = 60
            
            clarity_score = min(clarity_sigmoid, cap)
            
            # ë“±ê¸‰ ë§¤ê¸°ê¸°
            if clarity_score >= 70:
                grade = "High"
            elif clarity_score >= 40:
                grade = "Medium"
            else:
                grade = "Low"
            
            # íŠ¹ì§• íƒœê·¸
            characteristics = []
            if features['tilt_med'] > -6:
                characteristics.append("ë°ìŒ")
            else:
                characteristics.append("ì–´ë‘ì›€")
            
            if features['hnr_med'] > 15:
                characteristics.append("ê¹¨ë—í•¨")
            elif features['hnr_med'] < 10:
                characteristics.append("ê±°ì¹¨")
            else:
                characteristics.append("ë³´í†µ")
            
            if features['h1h2_med'] > 8:
                characteristics.append("ìˆ¨ì„ì„")
            
            if low_confidence:
                characteristics.append("Low confidence")
            
            return clarity_score, grade, characteristics
            
        except Exception as e:
            print(f"ìŠ¤ì½”ì–´ë§ ì˜¤ë¥˜: {e}")
            return 50.0, "Medium", ["ë¶„ì„ ì‹¤íŒ¨"]

def test_corrected_analyzer():
    """êµì •ëœ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    
    print("=" * 70)
    print("ğŸ¤ êµì •ëœ ë³´ì»¬ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    test_files = [
        (r"C:\Users\user\Desktop\vocals_extracted\ê¹€ë²”ìˆ˜_ê¹€ë²”ìˆ˜ - Dear Love_ë³´ì»¬.wav", "unknown"),
        (r"C:\Users\user\Documents\ì¹´ì¹´ì˜¤í†¡ ë°›ì€ íŒŒì¼\kakaotalk_1756474302376.m4a", "kakaotalk"),
        (r"C:\Users\user\Desktop\vocals_extracted\ë¡œì´í‚´_ë¡œì´í‚´ - As Is_ë³´ì»¬.wav", "unknown"),
        (r"C:\Users\user\Documents\ì†Œë¦¬ ë…¹ìŒ\ê°€ì„±_ê°€ì„±ë¹„ë¸Œë¼í† .wav", "unknown")
    ]
    
    analyzer = CorrectedVocalAnalyzer()
    results = []
    
    for audio_file, source_hint in test_files:
        if os.path.exists(audio_file):
            result = analyzer.analyze_vocal_file(audio_file, source_hint=source_hint)
            if 'clarity_score' in result:
                filename = os.path.basename(audio_file)
                results.append({
                    'file': filename[:20],
                    'score': result['clarity_score'],
                    'grade': result['grade'],
                    'characteristics': ', '.join(result['characteristics']),
                    'voiced_ratio': result['voiced_ratio']
                })
    
    # ìµœì¢… ê²°ê³¼
    if results:
        print("\n" + "=" * 80)
        print("ğŸ† ìµœì¢… ê²°ê³¼")
        print("=" * 80)
        
        print(f"\n{'íŒŒì¼':<22} | {'ì„ ëª…ë„':<6} | {'ë“±ê¸‰':<6} | {'ìœ ì„±ë¹„ìœ¨':<8} | {'íŠ¹ì§•'}")
        print("-" * 80)
        
        for r in results:
            print(f"{r['file']:<22} | {r['score']:<6.1f} | {r['grade']:<6} | {r['voiced_ratio']:<8.1%} | {r['characteristics']}")
        
        print(f"\nğŸ’¡ ì˜ˆìƒ ìˆœì„œ: ê¹€ë²”ìˆ˜ > kakaotalk > ë¡œì´í‚´ > ê°€ì„±")
        sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)
        actual_order = " > ".join([r['file'][:10] for r in sorted_results])
        print(f"ğŸ’¡ ì‹¤ì œ ìˆœì„œ: {actual_order}")

if __name__ == "__main__":
    test_corrected_analyzer()