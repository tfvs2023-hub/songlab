"""
ì§€ì‹ ì¦ë¥˜(Knowledge Distillation) - ì†Œí˜• MBTI ëª¨ë¸ â†’ ëŒ€í˜• ëª¨ë¸ ì „ë‹¬
ì „ë¬¸ê°€ ëª¨ë¸ì˜ ì§€ì‹ì„ ëŒ€í˜• ëª¨ë¸ì— íš¨ìœ¨ì ìœ¼ë¡œ ì „ë‹¬
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from typing import Dict, List, Tuple

class TeacherModel(nn.Module):
    """
    Teacher ëª¨ë¸ (ê¸°ì¡´ í•™ìŠµëœ ì†Œí˜• MBTI ëª¨ë¸)
    """
    def __init__(self, model_path: str):
        super().__init__()
        from train_mbti_model import MBTIVocalNet
        
        self.model = MBTIVocalNet()
        checkpoint = torch.load(model_path, map_location='cpu')
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        # TeacherëŠ” ê³ ì • (í•™ìŠµí•˜ì§€ ì•ŠìŒ)
        for param in self.model.parameters():
            param.requires_grad = False
    
    def forward(self, x):
        return self.model(x)
    
    def get_soft_targets(self, x, temperature=3.0):
        """
        ì†Œí”„íŠ¸ íƒ€ê²Ÿ ìƒì„± (ì˜¨ë„ ìŠ¤ì¼€ì¼ë§)
        """
        with torch.no_grad():
            logits = self.model(x)
            # ì˜¨ë„ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ ìŠ¤ì¼€ì¼ë§
            soft_targets = torch.softmax(logits / temperature, dim=1)
        return soft_targets


class StudentModel(nn.Module):
    """
    Student ëª¨ë¸ (ëŒ€í˜• íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜)
    """
    def __init__(self, input_size=37, hidden_size=1024, num_layers=8):
        super().__init__()
        
        # ëŒ€í˜• íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=16,
            dim_feedforward=hidden_size * 4,
            dropout=0.1,
            activation='gelu'
        )
        
        self.input_projection = nn.Linear(input_size, hidden_size)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        # 4ì¶• MBTI ì¶œë ¥ í—¤ë“œ
        self.output_head = nn.Sequential(
            nn.Linear(hidden_size, 512),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(256, 4),  # brightness, thickness, clarity, power
            nn.Tanh()
        )
    
    def forward(self, x):
        # [batch_size, input_size] -> [batch_size, 1, hidden_size]
        x = self.input_projection(x).unsqueeze(1)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ì²˜ë¦¬
        x = self.transformer(x.transpose(0, 1))  # [1, batch_size, hidden_size]
        x = self.layer_norm(x.squeeze(0))  # [batch_size, hidden_size]
        
        # MBTI 4ì¶• ì¶œë ¥
        return self.output_head(x)


class DistillationLoss(nn.Module):
    """
    ì§€ì‹ ì¦ë¥˜ ì†ì‹¤ í•¨ìˆ˜
    """
    def __init__(self, temperature=3.0, alpha=0.7):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha  # teacher loss ê°€ì¤‘ì¹˜
        self.mse_loss = nn.MSELoss()
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
    
    def forward(self, student_outputs, teacher_outputs, true_labels):
        # 1. í•˜ë“œ íƒ€ê²Ÿ ì†ì‹¤ (ì‹¤ì œ ë¼ë²¨ê³¼ì˜ MSE)
        hard_loss = self.mse_loss(student_outputs, true_labels)
        
        # 2. ì†Œí”„íŠ¸ íƒ€ê²Ÿ ì†ì‹¤ (teacherì™€ì˜ KL divergence)
        student_soft = torch.log_softmax(student_outputs / self.temperature, dim=1)
        teacher_soft = torch.softmax(teacher_outputs / self.temperature, dim=1)
        soft_loss = self.kl_loss(student_soft, teacher_soft) * (self.temperature ** 2)
        
        # 3. ê°€ì¤‘ ê²°í•©
        total_loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss
        
        return total_loss, hard_loss, soft_loss


def distill_knowledge(teacher_model_path: str, 
                     train_loader: DataLoader,
                     val_loader: DataLoader,
                     epochs: int = 50):
    """
    ì§€ì‹ ì¦ë¥˜ í•™ìŠµ
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Teacherì™€ Student ëª¨ë¸ ì´ˆê¸°í™”
    teacher = TeacherModel(teacher_model_path).to(device)
    student = StudentModel().to(device)
    
    # ì†ì‹¤í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
    distill_criterion = DistillationLoss(temperature=4.0, alpha=0.8)
    optimizer = optim.AdamW(student.parameters(), lr=1e-4, weight_decay=0.01)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    print(f"ğŸ“ ì§€ì‹ ì¦ë¥˜ ì‹œì‘")
    print(f"   Teacher: {sum(p.numel() for p in teacher.parameters() if p.requires_grad)}M params")
    print(f"   Student: {sum(p.numel() for p in student.parameters() if p.requires_grad)/1e6:.1f}M params")
    
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        # í•™ìŠµ í˜ì´ì¦ˆ
        student.train()
        total_loss, total_hard, total_soft = 0, 0, 0
        
        for features, labels in train_loader:
            features, labels = features.to(device), labels.to(device)
            
            # Teacherì˜ ì˜ˆì¸¡ (ê³ ì •)
            with torch.no_grad():
                teacher_outputs = teacher(features)
            
            # Studentì˜ ì˜ˆì¸¡
            student_outputs = student(features)
            
            # ì§€ì‹ ì¦ë¥˜ ì†ì‹¤ ê³„ì‚°
            loss, hard_loss, soft_loss = distill_criterion(
                student_outputs, teacher_outputs, labels
            )
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            total_hard += hard_loss.item()
            total_soft += soft_loss.item()
        
        scheduler.step()
        
        # ê²€ì¦
        student.eval()
        val_loss = 0
        with torch.no_grad():
            for features, labels in val_loader:
                features, labels = features.to(device), labels.to(device)
                
                teacher_outputs = teacher(features)
                student_outputs = student(features)
                
                loss, _, _ = distill_criterion(student_outputs, teacher_outputs, labels)
                val_loss += loss.item()
        
        avg_train_loss = total_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        avg_hard_loss = total_hard / len(train_loader)
        avg_soft_loss = total_soft / len(train_loader)
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}]")
            print(f"   Total: {avg_train_loss:.4f}, Hard: {avg_hard_loss:.4f}, Soft: {avg_soft_loss:.4f}")
            print(f"   Val: {avg_val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save({
                'epoch': epoch,
                'student_state_dict': student.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': avg_val_loss,
            }, 'distilled_large_model.pth')
    
    print(f"\nğŸ¯ ì§€ì‹ ì¦ë¥˜ ì™„ë£Œ! ëŒ€í˜• ëª¨ë¸ ì €ì¥ë¨")
    return student


class HybridMBTISystem:
    """
    í•˜ì´ë¸Œë¦¬ë“œ MBTI ì‹œìŠ¤í…œ (ì†Œí˜• + ëŒ€í˜• ëª¨ë¸ ì•™ìƒë¸”)
    """
    def __init__(self, teacher_path: str, student_path: str):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Teacher ëª¨ë¸ (ë¹ ë¥´ê³  ì •í™•)
        self.teacher = TeacherModel(teacher_path).to(self.device)
        
        # Student ëª¨ë¸ (ëŒ€í˜•, í‘œí˜„ë ¥ í’ë¶€)
        self.student = StudentModel().to(self.device)
        checkpoint = torch.load(student_path, map_location=self.device)
        self.student.load_state_dict(checkpoint['student_state_dict'])
        self.student.eval()
    
    def predict(self, features: torch.Tensor, use_ensemble=True) -> Dict:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ (ì•™ìƒë¸” ë˜ëŠ” ë‹¨ì¼)
        """
        features = features.to(self.device)
        
        with torch.no_grad():
            # Teacher ì˜ˆì¸¡ (ë¹ ë¦„)
            teacher_pred = self.teacher(features).squeeze().cpu().numpy()
            
            if use_ensemble:
                # Student ì˜ˆì¸¡ (ì •êµí•¨)
                student_pred = self.student(features).squeeze().cpu().numpy()
                
                # ê°€ì¤‘ ì•™ìƒë¸” (Teacher 70%, Student 30%)
                ensemble_pred = teacher_pred * 0.7 + student_pred * 0.3
                predictions = ensemble_pred * 100  # -100 ~ 100 ë²”ìœ„
            else:
                predictions = teacher_pred * 100
        
        return {
            'brightness': int(predictions[0]),
            'thickness': int(predictions[1]),
            'clarity': int(predictions[2]),
            'power': int(predictions[3]),
            'confidence': 0.95 if use_ensemble else 0.85
        }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ§  MBTI ì§€ì‹ ì¦ë¥˜ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    print("1ë‹¨ê³„: ì†Œí˜• ì „ë¬¸ê°€ ëª¨ë¸ í•™ìŠµ")
    print("2ë‹¨ê³„: ì§€ì‹ ì¦ë¥˜ë¡œ ëŒ€í˜• ëª¨ë¸ í•™ìŠµ") 
    print("3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹œìŠ¤í…œ êµ¬ì¶•")
    print("\nğŸ’¡ ì¥ì :")
    print("   â€¢ ì†Œí˜• ëª¨ë¸: ë¹ ë¥¸ ì¶”ë¡  ì†ë„")
    print("   â€¢ ëŒ€í˜• ëª¨ë¸: í’ë¶€í•œ í‘œí˜„ë ¥")
    print("   â€¢ ì•™ìƒë¸”: ìµœê³  ì •í™•ë„")
    
    # distill_knowledge('best_mbti_model.pth', train_loader, val_loader)